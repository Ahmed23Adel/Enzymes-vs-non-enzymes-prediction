{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "7S5pCNhjdTDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0\"\n",
        "!pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "!pip install torch_geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YNFUo-frHcA",
        "outputId": "d7778ca0-e9c8-4a87-ca64-d9b8dce256db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.1.0+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m486.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.16.0+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.1.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu118) (2025.3.2)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu118) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu118) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu118) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0+cu118) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu118) (2025.6.15)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0+cu118) (1.3.0)\n",
            "Installing collected packages: triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed torch-2.1.0+cu118 torchaudio-2.1.0+cu118 torchvision-0.16.0+cu118 triton-2.1.0\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.6.15)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYXDrSjpzw1l",
        "outputId": "4e050a97-dd7c-4563-c635-80281d7fd3e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.5.1)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-25.5.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (1.6.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.6.0)\n",
            "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyaml-25.5.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-25.5.0 scikit-optimize-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "irJ2j1XidWkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GraphSAGE, GCNConv, GATConv, GINConv, LayerNorm, BatchNorm\n",
        "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool, global_sort_pool\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn.aggr import SumAggregation, MeanAggregation, MaxAggregation, StdAggregation\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import pickle\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import warnings\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from sklearn.model_selection import ParameterSampler\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Bayesian Optimization imports\n",
        "try:\n",
        "    from skopt import gp_minimize\n",
        "    from skopt.space import Real, Integer, Categorical\n",
        "    from skopt.utils import use_named_args\n",
        "    from skopt.acquisition import gaussian_ei\n",
        "    BAYESIAN_OPT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Warning: scikit-optimize not available. Please install with: pip install scikit-optimize\")\n",
        "    BAYESIAN_OPT_AVAILABLE = False\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv, global_mean_pool, global_max_pool, global_add_pool\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "nyOuJRrNdToc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.prune as prune\n",
        "from torch_geometric.nn import GINConv, BatchNorm, LayerNorm\n",
        "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import time\n",
        "import psutil\n",
        "import gc\n",
        "from copy import deepcopy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "iO93oAORautI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurations"
      ],
      "metadata": {
        "id": "zyxkR2NEdZd_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PESMoivndNoY"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "Aku_DFGWdcdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c0b9ed-04e9-4c6c-d971-e8f5da8c4c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the dataset"
      ],
      "metadata": {
        "id": "P7BPFWm7ddZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Nodes**: Amino acids.\n",
        "2. **Edges**: Connections between amino acids that are within 6 Ångströms of each other.\n",
        "3. **Labels**: Binary classification indicating whether a protein is an enzyme or not."
      ],
      "metadata": {
        "id": "MGJryZSZgCqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clarification:\n",
        "1. The 89 features are node features, not graph-level features (Structural information, Chemical properties)\n"
      ],
      "metadata": {
        "id": "cI_un1ynievv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading DD dataset...\")\n",
        "dataset = TUDataset(root='/tmp/DD', name='DD')\n",
        "\n",
        "print(f\"Dataset: {dataset}\")\n",
        "print(f\"Number of graphs: {len(dataset)}\")\n",
        "print(f\"Number of features: {dataset.num_features}\")\n",
        "print(f\"Number of classes: {dataset.num_classes}\")\n",
        "\n",
        "# Get first graph for exploration\n",
        "data = dataset[0]\n",
        "print(f\"\\nFirst graph:\")\n",
        "print(f\"Number of nodes: {data.x.shape[0]}\")\n",
        "print(f\"Number of edges: {data.edge_index.shape[1]}\")\n",
        "print(f\"Node feature shape: {data.x.shape}\")\n",
        "print(f\"Label: {data.y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyAVzeETdfBv",
        "outputId": "cb8638d8-4354-4b47-9ac2-f17d092c8350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DD dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/DD.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: DD(1178)\n",
            "Number of graphs: 1178\n",
            "Number of features: 89\n",
            "Number of classes: 2\n",
            "\n",
            "First graph:\n",
            "Number of nodes: 327\n",
            "Number of edges: 1798\n",
            "Node feature shape: torch.Size([327, 89])\n",
            "Label: tensor([0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "eaXGj4jQkddj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = [data for data in dataset]"
      ],
      "metadata": {
        "id": "Yg-jMRoYkSM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## splitting"
      ],
      "metadata": {
        "id": "iudRKI4Wkitm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "labels = [data.y.item() for data in data_list]\n",
        "all_indices = list(range(len(data_list)))\n",
        "\n",
        "# First split: 60% train, 40% (val+test)\n",
        "train_idx, temp_idx = train_test_split(\n",
        "    all_indices,\n",
        "    test_size=0.4,\n",
        "    stratify=labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Second split: 20% val, 20% test from the remaining 40%\n",
        "# So test_size=0.5 here means 50% of the 40% => 20% of the total\n",
        "temp_labels = [labels[i] for i in temp_idx]\n",
        "val_idx, test_idx = train_test_split(\n",
        "    temp_idx,\n",
        "    test_size=0.5,\n",
        "    stratify=temp_labels,\n",
        "    random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "fRZiBa0TkgV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train set size: {len(train_idx)}\")\n",
        "print(f\"Validation set size: {len(val_idx)}\")\n",
        "print(f\"Test set size: {len(test_idx)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPW-ej9_kjZW",
        "outputId": "ffa2a9e8-280f-436a-f9b2-a8bb20707916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 706\n",
            "Validation set size: 236\n",
            "Test set size: 236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = [data_list[i] for i in train_idx]\n",
        "val_dataset = [data_list[i] for i in val_idx]\n",
        "test_dataset = [data_list[i] for i in test_idx]"
      ],
      "metadata": {
        "id": "BfEnnK2tklLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data loaders"
      ],
      "metadata": {
        "id": "Pkfh9hWAlGUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"\\nBatch size: {batch_size}\")\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of validation batches: {len(val_loader)}\")\n",
        "print(f\"Number of test batches: {len(test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WdKL_Q3lDf5",
        "outputId": "76bab4c5-dc32-4aea-8336-148fbd478ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch size: 32\n",
            "Number of training batches: 23\n",
            "Number of validation batches: 8\n",
            "Number of test batches: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING UTILITIES"
      ],
      "metadata": {
        "id": "l_hPSXoJ0MKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"Get current memory usage\"\"\"\n",
        "    process = psutil.Process()\n",
        "    cpu_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
        "    gpu_memory = 0\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB\n",
        "    return cpu_memory, gpu_memory\n",
        "\n",
        "def measure_inference_time(model, loader, device, num_samples=100):\n",
        "    model.eval()\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(loader):\n",
        "            if i * batch.y.size(0) >= num_samples:\n",
        "                break\n",
        "            batch = batch.to(device)\n",
        "            start_time = time.time()\n",
        "            _ = model(batch.x, batch.edge_index, batch.batch)\n",
        "            end_time = time.time()\n",
        "            batch_time = (end_time - start_time) / batch.y.size(0)\n",
        "            times.append(batch_time)\n",
        "    return np.mean(times) * 1000  # Convert to milliseconds\n"
      ],
      "metadata": {
        "id": "UfrhQWcS0O7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition: GINModel"
      ],
      "metadata": {
        "id": "mUozD0R160l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GINModel(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_classes, num_layers,\n",
        "                 dropout=0.3, global_pool='mean', eps=0.0, train_eps=False,\n",
        "                 batch_norm=True, layer_norm=False):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.batch_norms = nn.ModuleList() if batch_norm else None\n",
        "        self.layer_norms = nn.ModuleList() if layer_norm else None\n",
        "\n",
        "        # First layer\n",
        "        mlp = nn.Sequential(\n",
        "            nn.Linear(num_features, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        self.convs.append(GINConv(mlp, eps=eps, train_eps=train_eps))\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            mlp = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim)\n",
        "            )\n",
        "            self.convs.append(GINConv(mlp, eps=eps, train_eps=train_eps))\n",
        "\n",
        "        # Last layer\n",
        "        if num_layers > 1:\n",
        "            mlp = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim)\n",
        "            )\n",
        "            self.convs.append(GINConv(mlp, eps=eps, train_eps=train_eps))\n",
        "\n",
        "        # Normalization layers\n",
        "        if batch_norm:\n",
        "            for _ in range(num_layers):\n",
        "                self.batch_norms.append(BatchNorm(hidden_dim))\n",
        "        if layer_norm:\n",
        "            for _ in range(num_layers):\n",
        "                self.layer_norms.append(LayerNorm(hidden_dim))\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Global pooling\n",
        "        self.global_pool = {\n",
        "            'mean': global_mean_pool,\n",
        "            'max': global_max_pool,\n",
        "            'add': global_add_pool\n",
        "        }[global_pool]\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, edge_index)\n",
        "            if self.batch_norms:\n",
        "                x = self.batch_norms[i](x)\n",
        "            if self.layer_norms:\n",
        "                x = self.layer_norms[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Last layer\n",
        "        if len(self.convs) > 0:\n",
        "            x = self.convs[-1](x, edge_index)\n",
        "            if self.batch_norms and len(self.batch_norms) > len(self.convs) - 1:\n",
        "                x = self.batch_norms[-1](x)\n",
        "            if self.layer_norms and len(self.layer_norms) > len(self.convs) - 1:\n",
        "                x = self.layer_norms[-1](x)\n",
        "\n",
        "        x = self.global_pool(x, batch)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "shf4_tR87K9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model):\n",
        "    \"\"\"Calculate model size in MB\"\"\"\n",
        "    param_size = 0\n",
        "    buffer_size = 0\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "    size_mb = (param_size + buffer_size) / 1024 / 1024\n",
        "    return size_mb"
      ],
      "metadata": {
        "id": "KYyV8NL3j-O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count total and non-zero parameters\"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    # Count non-zero parameters (after pruning)\n",
        "    non_zero_params = 0\n",
        "    for p in model.parameters():\n",
        "        if hasattr(p, 'weight_mask'):\n",
        "            # Pruned parameter\n",
        "            non_zero_params += torch.sum(p.weight_mask).item()\n",
        "        else:\n",
        "            # Regular parameter\n",
        "            non_zero_params += torch.sum(p != 0).item()\n",
        "\n",
        "    sparsity = (total_params - non_zero_params) / total_params * 100\n",
        "    return total_params, non_zero_params, sparsity"
      ],
      "metadata": {
        "id": "_NBClTonkBtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_flops_reduction(original_params, pruned_params):\n",
        "    \"\"\"Estimate FLOPS reduction based on parameter reduction\"\"\"\n",
        "    return (original_params - pruned_params) / original_params * 100"
      ],
      "metadata": {
        "id": "sjkJxPWHkEXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evalutation of model"
      ],
      "metadata": {
        "id": "8bNtMSt3C4Oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            pred = model(batch.x, batch.edge_index, batch.batch).argmax(dim=1)\n",
        "            correct += (pred == batch.y).sum().item()\n",
        "            total += batch.y.size(0)\n",
        "    return correct / total\n",
        "\n",
        "def evaluate_detailed(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            pred = model(batch.x, batch.edge_index, batch.batch).argmax(dim=1)\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy_score(all_labels, all_preds),\n",
        "        'f1': f1_score(all_labels, all_preds, average='binary'),\n",
        "        'precision': precision_score(all_labels, all_preds, average='binary'),\n",
        "        'recall': recall_score(all_labels, all_preds, average='binary'),\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Zl2nd9wxC5su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    inference_times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            start_time = time.time()\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            inference_time = (time.time() - start_time) * 1000  # ms\n",
        "\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == batch.y).sum().item()\n",
        "            total += batch.y.size(0)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(batch.y.cpu().numpy())\n",
        "            inference_times.append(inference_time)\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'inference_time_ms': avg_inference_time\n",
        "    }\n"
      ],
      "metadata": {
        "id": "NgWw8ITqkH6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and evalutation function"
      ],
      "metadata": {
        "id": "85vq2eQp7W4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate(model, train_loader, val_loader, test_loader, device, epochs=100, plot_training=False):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # For plotting and peak memory tracking\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    epoch_times = []\n",
        "    peak_train_cpu = peak_train_gpu = 0\n",
        "    peak_inf_cpu = peak_inf_gpu = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # Train with peak memory tracking\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = criterion(out, batch.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Track peak memory during training\n",
        "        cpu_mem, gpu_mem = get_memory_usage()\n",
        "        peak_train_cpu = max(peak_train_cpu, cpu_mem)\n",
        "        peak_train_gpu = max(peak_train_gpu, gpu_mem)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        epoch_times.append(epoch_time)\n",
        "\n",
        "        # Validate every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            val_acc = evaluate(model, val_loader, device)\n",
        "            train_acc = evaluate(model, train_loader, device)\n",
        "            val_loss = evaluate_loss(model, val_loader, device, criterion)\n",
        "\n",
        "            train_losses.append(epoch_loss / len(train_loader))\n",
        "            val_losses.append(val_loss)\n",
        "            train_accs.append(train_acc)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= 5:  # Early stopping\n",
        "                break\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Plot training curves for best model\n",
        "    if plot_training:\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        epochs_range = range(0, len(train_losses) * 10, 10)\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(epochs_range, train_losses, 'b-', label='Train Loss')\n",
        "        plt.plot(epochs_range, val_losses, 'r-', label='Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Train vs Validation Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(epochs_range, train_accs, 'b-', label='Train Accuracy')\n",
        "        plt.plot(epochs_range, val_accs, 'r-', label='Validation Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Train vs Validation Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(epochs_range, train_losses, 'b-', label='Train Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    # Peak memory during inference\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = batch.to(device)\n",
        "            _ = model(batch.x, batch.edge_index, batch.batch)\n",
        "            cpu_mem, gpu_mem = get_memory_usage()\n",
        "            peak_inf_cpu = max(peak_inf_cpu, cpu_mem)\n",
        "            peak_inf_gpu = max(peak_inf_gpu, gpu_mem)\n",
        "\n",
        "    metrics = evaluate_detailed(model, test_loader, device)\n",
        "    inference_time = measure_inference_time(model, test_loader, device)\n",
        "\n",
        "    metrics.update({\n",
        "        'training_time': training_time,\n",
        "        'avg_epoch_time': np.mean(epoch_times),\n",
        "        'parameters': count_parameters(model),\n",
        "        'peak_train_cpu_mb': peak_train_cpu,\n",
        "        'peak_train_gpu_mb': peak_train_gpu,\n",
        "        'peak_inf_cpu_mb': peak_inf_cpu,\n",
        "        'peak_inf_gpu_mb': peak_inf_gpu,\n",
        "        'inference_time_ms': inference_time\n",
        "    })\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Helper function for validation loss\n"
      ],
      "metadata": {
        "id": "L_6P8-fq7V-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_loss(model, loader, device, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = criterion(out, batch.y)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ],
      "metadata": {
        "id": "WV8r7QfoCxIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimize the model"
      ],
      "metadata": {
        "id": "tgU8H5pu95OS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import time\n",
        "import gc\n",
        "import psutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from torch_geometric.data import Data, Batch\n",
        "\n",
        "class PruningComparator:\n",
        "    def __init__(self, model_config, device=None):\n",
        "        self.model_config = model_config\n",
        "        # Auto-detect device if not specified\n",
        "        if device is None:\n",
        "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        else:\n",
        "            self.device = torch.device(device)\n",
        "\n",
        "        print(f\"PruningComparator initialized with device: {self.device}\")\n",
        "        self.results = {}\n",
        "\n",
        "    def ensure_device_consistency(self, model, data):\n",
        "        \"\"\"Ensure model and data are on the same device\"\"\"\n",
        "        # Move model to device\n",
        "        model = model.to(self.device)\n",
        "\n",
        "        # Handle different data types\n",
        "        if isinstance(data, (Data, Batch)):\n",
        "            data = data.to(self.device)\n",
        "        elif hasattr(data, 'to') and callable(getattr(data, 'to')):\n",
        "            # For objects that have a .to() method\n",
        "            data = data.to(self.device)\n",
        "        elif hasattr(data, '__dict__'):\n",
        "            # For custom objects with attributes, move tensor attributes to device\n",
        "            for attr_name in dir(data):\n",
        "                if not attr_name.startswith('_'):\n",
        "                    attr_value = getattr(data, attr_name)\n",
        "                    if torch.is_tensor(attr_value):\n",
        "                        setattr(data, attr_name, attr_value.to(self.device))\n",
        "        elif isinstance(data, tuple):\n",
        "            data = tuple(item.to(self.device) if torch.is_tensor(item) else item for item in data)\n",
        "        elif torch.is_tensor(data):\n",
        "            data = data.to(self.device)\n",
        "\n",
        "        return model, data\n",
        "\n",
        "    def get_model_size(self, model):\n",
        "        \"\"\"Calculate model size in MB\"\"\"\n",
        "        param_size = 0\n",
        "        buffer_size = 0\n",
        "\n",
        "        for param in model.parameters():\n",
        "            param_size += param.nelement() * param.element_size()\n",
        "\n",
        "        for buffer in model.buffers():\n",
        "            buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "        model_size = (param_size + buffer_size) / 1024 / 1024\n",
        "        return model_size\n",
        "\n",
        "    def count_parameters(self, model):\n",
        "        \"\"\"Count total and non-zero parameters\"\"\"\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "        # Count non-zero parameters (for pruned models)\n",
        "        non_zero_params = 0\n",
        "        for p in model.parameters():\n",
        "            if hasattr(p, 'weight_mask'):\n",
        "                non_zero_params += torch.sum(p.weight_mask).item()\n",
        "            else:\n",
        "                non_zero_params += torch.sum(p != 0).item()\n",
        "\n",
        "        return total_params, non_zero_params\n",
        "\n",
        "    def get_memory_usage(self):\n",
        "        \"\"\"Get current memory usage\"\"\"\n",
        "        process = psutil.Process()\n",
        "        cpu_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
        "\n",
        "        gpu_memory = 0\n",
        "        if torch.cuda.is_available() and self.device.type == 'cuda':\n",
        "            gpu_memory = torch.cuda.memory_allocated(self.device) / 1024 / 1024  # MB\n",
        "\n",
        "        return cpu_memory, gpu_memory\n",
        "\n",
        "    def measure_inference_time(self, model, sample_data, num_runs=100):\n",
        "        \"\"\"Measure inference time with device consistency\"\"\"\n",
        "        # Ensure everything is on the same device\n",
        "        model, sample_data = self.ensure_device_consistency(model, sample_data)\n",
        "        model.eval()\n",
        "\n",
        "        # Handle different data formats\n",
        "        if isinstance(sample_data, (Data, Batch)):\n",
        "            x, edge_index, batch = sample_data.x, sample_data.edge_index, sample_data.batch\n",
        "        else:\n",
        "            x, edge_index, batch = sample_data\n",
        "\n",
        "        # Ensure all tensors are on the correct device\n",
        "        x = x.to(self.device)\n",
        "        edge_index = edge_index.to(self.device)\n",
        "        if batch is not None:\n",
        "            batch = batch.to(self.device)\n",
        "\n",
        "        # Warmup runs\n",
        "        with torch.no_grad():\n",
        "            for _ in range(10):\n",
        "                try:\n",
        "                    if batch is not None:\n",
        "                        _ = model(x, edge_index, batch)\n",
        "                    else:\n",
        "                        _ = model(x, edge_index)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning during warmup: {e}\")\n",
        "                    break\n",
        "\n",
        "        # Measure inference time\n",
        "        if self.device.type == 'cuda':\n",
        "            torch.cuda.synchronize(self.device)\n",
        "\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            for _ in range(num_runs):\n",
        "                try:\n",
        "                    if batch is not None:\n",
        "                        _ = model(x, edge_index, batch)\n",
        "                    else:\n",
        "                        _ = model(x, edge_index)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during inference timing: {e}\")\n",
        "                    return float('inf')\n",
        "\n",
        "        if self.device.type == 'cuda':\n",
        "            torch.cuda.synchronize(self.device)\n",
        "\n",
        "        end_time = time.time()\n",
        "        avg_time = (end_time - start_time) / num_runs * 1000  # ms\n",
        "        return avg_time\n",
        "\n",
        "    def unstructured_pruning(self, model, pruning_ratio=0.5):\n",
        "        \"\"\"Apply unstructured magnitude-based pruning\"\"\"\n",
        "        model_pruned = deepcopy(model).to(self.device)\n",
        "\n",
        "        # Collect all linear layers\n",
        "        modules_to_prune = []\n",
        "        for name, module in model_pruned.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                modules_to_prune.append((module, 'weight'))\n",
        "\n",
        "        if not modules_to_prune:\n",
        "            print(\"Warning: No Linear layers found for pruning\")\n",
        "            return model_pruned\n",
        "\n",
        "        # Apply global unstructured pruning\n",
        "        prune.global_unstructured(\n",
        "            modules_to_prune,\n",
        "            pruning_method=prune.L1Unstructured,\n",
        "            amount=pruning_ratio,\n",
        "        )\n",
        "\n",
        "        # Make pruning permanent\n",
        "        for module, param_name in modules_to_prune:\n",
        "            prune.remove(module, param_name)\n",
        "\n",
        "        return model_pruned\n",
        "\n",
        "    def structured_pruning(self, model, pruning_ratio=0.5):\n",
        "        \"\"\"Apply structured pruning (remove entire neurons/channels)\"\"\"\n",
        "        model_pruned = deepcopy(model).to(self.device)\n",
        "\n",
        "        # Apply structured pruning to linear layers\n",
        "        for name, module in model_pruned.named_modules():\n",
        "            if isinstance(module, nn.Linear) and hasattr(module, 'weight'):\n",
        "                # Calculate L1 norm for each output neuron\n",
        "                weight = module.weight.data\n",
        "                l1_norm = torch.norm(weight, p=1, dim=1)\n",
        "\n",
        "                # Determine number of neurons to prune\n",
        "                num_neurons = weight.size(0)\n",
        "                num_to_prune = int(num_neurons * pruning_ratio)\n",
        "\n",
        "                if num_to_prune > 0 and num_to_prune < num_neurons:\n",
        "                    try:\n",
        "                        # Apply structured pruning\n",
        "                        prune.ln_structured(module, name='weight', amount=num_to_prune, n=1, dim=0)\n",
        "                        prune.remove(module, 'weight')\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Could not apply structured pruning to {name}: {e}\")\n",
        "\n",
        "        return model_pruned\n",
        "\n",
        "    def magnitude_based_pruning(self, model, pruning_ratio=0.5):\n",
        "        \"\"\"Apply magnitude-based pruning with different thresholds\"\"\"\n",
        "        model_pruned = deepcopy(model).to(self.device)\n",
        "\n",
        "        # Collect all weights\n",
        "        all_weights = []\n",
        "        for param in model_pruned.parameters():\n",
        "            if param.requires_grad:\n",
        "                all_weights.extend(param.data.abs().flatten().tolist())\n",
        "\n",
        "        if not all_weights:\n",
        "            print(\"Warning: No trainable parameters found\")\n",
        "            return model_pruned\n",
        "\n",
        "        # Calculate threshold based on magnitude\n",
        "        all_weights = torch.tensor(all_weights, device=self.device)\n",
        "        threshold = torch.quantile(all_weights, pruning_ratio)\n",
        "\n",
        "        # Apply pruning based on magnitude threshold\n",
        "        for param in model_pruned.parameters():\n",
        "            if param.requires_grad:\n",
        "                mask = param.data.abs() > threshold\n",
        "                param.data *= mask.float()\n",
        "\n",
        "        return model_pruned\n",
        "\n",
        "    def random_pruning(self, model, pruning_ratio=0.5):\n",
        "        \"\"\"Apply random pruning for comparison\"\"\"\n",
        "        model_pruned = deepcopy(model).to(self.device)\n",
        "\n",
        "        modules_to_prune = []\n",
        "        for name, module in model_pruned.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                modules_to_prune.append((module, 'weight'))\n",
        "\n",
        "        if not modules_to_prune:\n",
        "            print(\"Warning: No Linear layers found for random pruning\")\n",
        "            return model_pruned\n",
        "\n",
        "        # Apply random pruning\n",
        "        prune.global_unstructured(\n",
        "            modules_to_prune,\n",
        "            pruning_method=prune.RandomUnstructured,\n",
        "            amount=pruning_ratio,\n",
        "        )\n",
        "\n",
        "        # Make pruning permanent\n",
        "        for module, param_name in modules_to_prune:\n",
        "            prune.remove(module, param_name)\n",
        "\n",
        "        return model_pruned\n",
        "\n",
        "    def evaluate_model(self, model, test_loader, criterion):\n",
        "        \"\"\"Evaluate model performance with device consistency\"\"\"\n",
        "        model, _ = self.ensure_device_consistency(model, None)\n",
        "        model.eval()\n",
        "\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                # Move data to device - handle custom data objects\n",
        "                if hasattr(data, 'to') and callable(getattr(data, 'to')):\n",
        "                    data = data.to(self.device)\n",
        "                elif hasattr(data, '__dict__'):\n",
        "                    # For custom objects, move tensor attributes to device\n",
        "                    for attr_name in ['x', 'edge_index', 'batch', 'y']:\n",
        "                        if hasattr(data, attr_name):\n",
        "                            attr_value = getattr(data, attr_name)\n",
        "                            if torch.is_tensor(attr_value):\n",
        "                                setattr(data, attr_name, attr_value.to(self.device))\n",
        "\n",
        "                x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
        "\n",
        "                try:\n",
        "                    outputs = model(x, edge_index, batch)\n",
        "                    loss = criterion(outputs, y)\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += y.size(0)\n",
        "                    correct += (predicted == y).sum().item()\n",
        "\n",
        "                    all_preds.extend(predicted.cpu().numpy())\n",
        "                    all_labels.extend(y.cpu().numpy())\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during evaluation: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if total == 0:\n",
        "            return {\n",
        "                'accuracy': 0.0,\n",
        "                'loss': float('inf'),\n",
        "                'f1_score': 0.0,\n",
        "                'precision': 0.0,\n",
        "                'recall': 0.0\n",
        "            }\n",
        "\n",
        "        accuracy = correct / total\n",
        "        avg_loss = total_loss / len(test_loader) if len(test_loader) > 0 else float('inf')\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        try:\n",
        "            f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "            precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "            recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not calculate metrics: {e}\")\n",
        "            f1 = precision = recall = 0.0\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'loss': avg_loss,\n",
        "            'f1_score': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        }\n",
        "\n",
        "    def compare_pruning_methods(self, original_model, test_loader, sample_data,\n",
        "                              pruning_ratios=[0.1, 0.3, 0.5, 0.7, 0.9]):\n",
        "        \"\"\"Compare different pruning methods with device consistency\"\"\"\n",
        "        criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "\n",
        "        # Ensure original model is on correct device\n",
        "        original_model = original_model.to(self.device)\n",
        "\n",
        "        pruning_methods = {\n",
        "            'Original': lambda m, r: m,\n",
        "            'Unstructured': self.unstructured_pruning,\n",
        "            'Structured': self.structured_pruning,\n",
        "            'Magnitude-based': self.magnitude_based_pruning,\n",
        "            'Random': self.random_pruning\n",
        "        }\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for method_name, pruning_func in pruning_methods.items():\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Testing {method_name} Pruning\")\n",
        "            print(f\"{'='*50}\")\n",
        "\n",
        "            if method_name == 'Original':\n",
        "                ratios_to_test = [0.0]\n",
        "            else:\n",
        "                ratios_to_test = pruning_ratios\n",
        "\n",
        "            for ratio in ratios_to_test:\n",
        "                print(f\"\\nPruning ratio: {ratio}\")\n",
        "\n",
        "                try:\n",
        "                    # Apply pruning\n",
        "                    if method_name == 'Original':\n",
        "                        pruned_model = deepcopy(original_model).to(self.device)\n",
        "                    else:\n",
        "                        pruned_model = pruning_func(original_model, ratio)\n",
        "\n",
        "                    # Measure metrics\n",
        "                    start_time = time.time()\n",
        "\n",
        "                    # Model size and parameters\n",
        "                    model_size = self.get_model_size(pruned_model)\n",
        "                    total_params, non_zero_params = self.count_parameters(pruned_model)\n",
        "                    sparsity = 1 - (non_zero_params / total_params) if total_params > 0 else 0\n",
        "\n",
        "                    # Memory usage\n",
        "                    cpu_mem, gpu_mem = self.get_memory_usage()\n",
        "\n",
        "                    # Inference time\n",
        "                    inference_time = self.measure_inference_time(pruned_model, sample_data)\n",
        "\n",
        "                    # Model performance\n",
        "                    performance = self.evaluate_model(pruned_model, test_loader, criterion)\n",
        "\n",
        "                    # Compilation time\n",
        "                    compile_time = time.time() - start_time\n",
        "\n",
        "                    result = {\n",
        "                        'Method': method_name,\n",
        "                        'Pruning_Ratio': ratio,\n",
        "                        'Model_Size_MB': model_size,\n",
        "                        'Total_Parameters': total_params,\n",
        "                        'Non_Zero_Parameters': non_zero_params,\n",
        "                        'Sparsity': sparsity,\n",
        "                        'CPU_Memory_MB': cpu_mem,\n",
        "                        'GPU_Memory_MB': gpu_mem,\n",
        "                        'Inference_Time_ms': inference_time,\n",
        "                        'Compile_Time_s': compile_time,\n",
        "                        'Accuracy': performance['accuracy'],\n",
        "                        'F1_Score': performance['f1_score'],\n",
        "                        'Precision': performance['precision'],\n",
        "                        'Recall': performance['recall'],\n",
        "                        'Loss': performance['loss']\n",
        "                    }\n",
        "\n",
        "                    results.append(result)\n",
        "\n",
        "                    print(f\"  Model Size: {model_size:.2f} MB\")\n",
        "                    print(f\"  Sparsity: {sparsity:.2%}\")\n",
        "                    print(f\"  Accuracy: {performance['accuracy']:.4f}\")\n",
        "                    print(f\"  F1-Score: {performance['f1_score']:.4f}\")\n",
        "                    print(f\"  Inference Time: {inference_time:.2f} ms\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {method_name} with ratio {ratio}: {e}\")\n",
        "                    continue\n",
        "                finally:\n",
        "                    # Clean up\n",
        "                    if 'pruned_model' in locals():\n",
        "                        del pruned_model\n",
        "                    gc.collect()\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def create_visualizations(self, results_df):\n",
        "        \"\"\"Create comprehensive visualizations\"\"\"\n",
        "        if results_df.empty:\n",
        "            print(\"No results to visualize\")\n",
        "            return None\n",
        "\n",
        "        plt.style.use('default')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Accuracy vs Pruning Ratio\n",
        "        ax1 = plt.subplot(3, 3, 1)\n",
        "        for method in results_df['Method'].unique():\n",
        "            method_data = results_df[results_df['Method'] == method]\n",
        "            plt.plot(method_data['Pruning_Ratio'], method_data['Accuracy'],\n",
        "                    marker='o', linewidth=2, label=method)\n",
        "        plt.xlabel('Pruning Ratio')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Accuracy vs Pruning Ratio')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. Model Size vs Pruning Ratio\n",
        "        ax2 = plt.subplot(3, 3, 2)\n",
        "        for method in results_df['Method'].unique():\n",
        "            method_data = results_df[results_df['Method'] == method]\n",
        "            plt.plot(method_data['Pruning_Ratio'], method_data['Model_Size_MB'],\n",
        "                    marker='s', linewidth=2, label=method)\n",
        "        plt.xlabel('Pruning Ratio')\n",
        "        plt.ylabel('Model Size (MB)')\n",
        "        plt.title('Model Size vs Pruning Ratio')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. Inference Time vs Pruning Ratio\n",
        "        ax3 = plt.subplot(3, 3, 3)\n",
        "        for method in results_df['Method'].unique():\n",
        "            method_data = results_df[results_df['Method'] == method]\n",
        "            plt.plot(method_data['Pruning_Ratio'], method_data['Inference_Time_ms'],\n",
        "                    marker='^', linewidth=2, label=method)\n",
        "        plt.xlabel('Pruning Ratio')\n",
        "        plt.ylabel('Inference Time (ms)')\n",
        "        plt.title('Inference Time vs Pruning Ratio')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. Sparsity vs Accuracy\n",
        "        ax4 = plt.subplot(3, 3, 4)\n",
        "        for method in results_df['Method'].unique():\n",
        "            if method != 'Original':\n",
        "                method_data = results_df[results_df['Method'] == method]\n",
        "                plt.scatter(method_data['Sparsity'], method_data['Accuracy'],\n",
        "                           s=100, alpha=0.7, label=method)\n",
        "        plt.xlabel('Sparsity')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Sparsity vs Accuracy Trade-off')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 5. F1-Score Comparison\n",
        "        ax5 = plt.subplot(3, 3, 5)\n",
        "        pruning_50 = results_df[results_df['Pruning_Ratio'] == 0.5]\n",
        "        if not pruning_50.empty:\n",
        "            methods = pruning_50['Method'].tolist()\n",
        "            f1_scores = pruning_50['F1_Score'].tolist()\n",
        "            bars = plt.bar(methods, f1_scores, alpha=0.7, color=plt.cm.Set3(range(len(methods))))\n",
        "            plt.ylabel('F1-Score')\n",
        "            plt.title('F1-Score Comparison (50% Pruning)')\n",
        "            plt.xticks(rotation=45)\n",
        "            for bar, score in zip(bars, f1_scores):\n",
        "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                        f'{score:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # 6. Memory Efficiency\n",
        "        ax6 = plt.subplot(3, 3, 6)\n",
        "        for method in results_df['Method'].unique():\n",
        "            method_data = results_df[results_df['Method'] == method]\n",
        "            plt.plot(method_data['Pruning_Ratio'], method_data['CPU_Memory_MB'],\n",
        "                    marker='d', linewidth=2, label=method)\n",
        "        plt.xlabel('Pruning Ratio')\n",
        "        plt.ylabel('CPU Memory (MB)')\n",
        "        plt.title('Memory Usage vs Pruning Ratio')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 7. Accuracy vs Model Size (Efficiency Plot)\n",
        "        ax7 = plt.subplot(3, 3, 7)\n",
        "        for method in results_df['Method'].unique():\n",
        "            method_data = results_df[results_df['Method'] == method]\n",
        "            plt.scatter(method_data['Model_Size_MB'], method_data['Accuracy'],\n",
        "                       s=100, alpha=0.7, label=method)\n",
        "        plt.xlabel('Model Size (MB)')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Accuracy vs Model Size (Efficiency)')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 8. Comprehensive Performance Heatmap\n",
        "        ax8 = plt.subplot(3, 3, 8)\n",
        "        pivot_data = results_df.pivot_table(\n",
        "            values='Accuracy',\n",
        "            index='Method',\n",
        "            columns='Pruning_Ratio',\n",
        "            fill_value=np.nan\n",
        "        )\n",
        "        if not pivot_data.empty:\n",
        "            sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd',\n",
        "                       cbar_kws={'label': 'Accuracy'})\n",
        "        plt.title('Accuracy Heatmap by Method and Pruning Ratio')\n",
        "\n",
        "        # 9. Pareto Frontier (Accuracy vs Compression)\n",
        "        ax9 = plt.subplot(3, 3, 9)\n",
        "        original_data = results_df[results_df['Method'] == 'Original']\n",
        "        if not original_data.empty:\n",
        "            original_size = original_data['Model_Size_MB'].iloc[0]\n",
        "\n",
        "            for method in results_df['Method'].unique():\n",
        "                if method != 'Original':\n",
        "                    method_data = results_df[results_df['Method'] == method]\n",
        "                    compression_ratio = original_size / method_data['Model_Size_MB']\n",
        "                    plt.scatter(compression_ratio, method_data['Accuracy'],\n",
        "                               s=100, alpha=0.7, label=method)\n",
        "\n",
        "            plt.xlabel('Compression Ratio')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.title('Pareto Frontier: Accuracy vs Compression')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def generate_report(self, results_df):\n",
        "        \"\"\"Generate comprehensive comparison report\"\"\"\n",
        "        if results_df.empty:\n",
        "            print(\"No results to report\")\n",
        "            return results_df\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"COMPREHENSIVE PRUNING COMPARISON REPORT\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Overall statistics\n",
        "        print(f\"\\nTested {len(results_df['Method'].unique())} pruning methods\")\n",
        "        print(f\"Pruning ratios: {sorted(results_df['Pruning_Ratio'].unique())}\")\n",
        "\n",
        "        # Best performers at different pruning ratios\n",
        "        for ratio in [0.3, 0.5, 0.7]:\n",
        "            ratio_data = results_df[results_df['Pruning_Ratio'] == ratio]\n",
        "            if not ratio_data.empty:\n",
        "                best_accuracy = ratio_data.loc[ratio_data['Accuracy'].idxmax()]\n",
        "                efficiency_metric = ratio_data['Accuracy'] / ratio_data['Model_Size_MB']\n",
        "                best_efficiency = ratio_data.loc[efficiency_metric.idxmax()]\n",
        "\n",
        "                print(f\"\\n--- At {ratio*100}% Pruning ---\")\n",
        "                print(f\"Best Accuracy: {best_accuracy['Method']} \"\n",
        "                      f\"({best_accuracy['Accuracy']:.4f})\")\n",
        "                print(f\"Best Efficiency: {best_efficiency['Method']} \"\n",
        "                      f\"(Acc: {best_efficiency['Accuracy']:.4f}, \"\n",
        "                      f\"Size: {best_efficiency['Model_Size_MB']:.2f} MB)\")\n",
        "\n",
        "        # Method comparison summary\n",
        "        print(f\"\\n--- Method Summary ---\")\n",
        "        for method in results_df['Method'].unique():\n",
        "            method_data = results_df[results_df['Method'] == method]\n",
        "            avg_accuracy = method_data['Accuracy'].mean()\n",
        "            avg_size = method_data['Model_Size_MB'].mean()\n",
        "            avg_inference = method_data['Inference_Time_ms'].mean()\n",
        "\n",
        "            print(f\"{method}:\")\n",
        "            print(f\"  Avg Accuracy: {avg_accuracy:.4f}\")\n",
        "            print(f\"  Avg Model Size: {avg_size:.2f} MB\")\n",
        "            print(f\"  Avg Inference Time: {avg_inference:.2f} ms\")\n",
        "\n",
        "        # Detailed comparison table\n",
        "        print(f\"\\n--- Detailed Results ---\")\n",
        "        print(results_df.round(4).to_string(index=False))\n",
        "\n",
        "        return results_df"
      ],
      "metadata": {
        "id": "Yt5DFmUMa9zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import time\n",
        "import gc\n",
        "import psutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from copy import deepcopy\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from torch_geometric.data import Data, Batch\n",
        "\n",
        "class PruningComparator:\n",
        "    def __init__(self, model_config, device=None):\n",
        "        self.model_config = model_config\n",
        "        # Auto-detect device if not specified\n",
        "        if device is None:\n",
        "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        else:\n",
        "            self.device = torch.device(device)\n",
        "\n",
        "        print(f\"PruningComparator initialized with device: {self.device}\")\n",
        "        self.results = {}\n",
        "\n",
        "    def ensure_device_consistency(self, model, data):\n",
        "        \"\"\"Ensure model and data are on the same device\"\"\"\n",
        "        # Move model to device\n",
        "        model = model.to(self.device)\n",
        "\n",
        "        # Handle different data types\n",
        "        if isinstance(data, (Data, Batch)):\n",
        "            data = data.to(self.device)\n",
        "        elif hasattr(data, 'to') and callable(getattr(data, 'to')):\n",
        "            # For objects that have a .to() method\n",
        "            data = data.to(self.device)\n",
        "        elif hasattr(data, '__dict__'):\n",
        "            # For custom objects with attributes, move tensor attributes to device\n",
        "            for attr_name in dir(data):\n",
        "                if not attr_name.startswith('_'):\n",
        "                    attr_value = getattr(data, attr_name)\n",
        "                    if torch.is_tensor(attr_value):\n",
        "                        setattr(data, attr_name, attr_value.to(self.device))\n",
        "        elif isinstance(data, tuple):\n",
        "            data = tuple(item.to(self.device) if torch.is_tensor(item) else item for item in data)\n",
        "        elif torch.is_tensor(data):\n",
        "            data = data.to(self.device)\n",
        "\n",
        "        return model, data\n",
        "\n",
        "    def get_model_size(self, model):\n",
        "        \"\"\"Calculate model size in MB\"\"\"\n",
        "        param_size = 0\n",
        "        buffer_size = 0\n",
        "\n",
        "        for param in model.parameters():\n",
        "            param_size += param.nelement() * param.element_size()\n",
        "\n",
        "        for buffer in model.buffers():\n",
        "            buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "        model_size = (param_size + buffer_size) / 1024 / 1024\n",
        "        return model_size\n",
        "\n",
        "    def count_parameters(self, model):\n",
        "        \"\"\"Count total and non-zero parameters\"\"\"\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "        # Count non-zero parameters (for pruned models)\n",
        "        non_zero_params = 0\n",
        "        for p in model.parameters():\n",
        "            if hasattr(p, 'weight_mask'):\n",
        "                non_zero_params += torch.sum(p.weight_mask).item()\n",
        "            else:\n",
        "                non_zero_params += torch.sum(p != 0).item()\n",
        "\n",
        "        return total_params, non_zero_params\n",
        "\n",
        "    def get_memory_usage(self):\n",
        "        \"\"\"Get current memory usage\"\"\"\n",
        "        process = psutil.Process()\n",
        "        cpu_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
        "\n",
        "        gpu_memory = 0\n",
        "        if torch.cuda.is_available() and self.device.type == 'cuda':\n",
        "            gpu_memory = torch.cuda.memory_allocated(self.device) / 1024 / 1024  # MB\n",
        "\n",
        "        return cpu_memory, gpu_memory\n",
        "\n",
        "    def measure_inference_time(self, model, sample_data, num_runs=100):\n",
        "        \"\"\"Measure inference time with device consistency\"\"\"\n",
        "        # Ensure everything is on the same device\n",
        "        model, sample_data = self.ensure_device_consistency(model, sample_data)\n",
        "        model.eval()\n",
        "\n",
        "        # Handle different data formats\n",
        "        if isinstance(sample_data, (Data, Batch)):\n",
        "            x, edge_index, batch = sample_data.x, sample_data.edge_index, sample_data.batch\n",
        "        else:\n",
        "            x, edge_index, batch = sample_data\n",
        "\n",
        "        # Ensure all tensors are on the correct device\n",
        "        x = x.to(self.device)\n",
        "        edge_index = edge_index.to(self.device)\n",
        "        if batch is not None:\n",
        "            batch = batch.to(self.device)\n",
        "\n",
        "        # Warmup runs\n",
        "        with torch.no_grad():\n",
        "            for _ in range(10):\n",
        "                try:\n",
        "                    if batch is not None:\n",
        "                        _ = model(x, edge_index, batch)\n",
        "                    else:\n",
        "                        _ = model(x, edge_index)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning during warmup: {e}\")\n",
        "                    break\n",
        "\n",
        "        # Measure inference time\n",
        "        if self.device.type == 'cuda':\n",
        "            torch.cuda.synchronize(self.device)\n",
        "\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            for _ in range(num_runs):\n",
        "                try:\n",
        "                    if batch is not None:\n",
        "                        _ = model(x, edge_index, batch)\n",
        "                    else:\n",
        "                        _ = model(x, edge_index)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during inference timing: {e}\")\n",
        "                    return float('inf')\n",
        "\n",
        "        if self.device.type == 'cuda':\n",
        "            torch.cuda.synchronize(self.device)\n",
        "\n",
        "        end_time = time.time()\n",
        "        avg_time = (end_time - start_time) / num_runs * 1000  # ms\n",
        "        return avg_time\n",
        "\n",
        "    def unstructured_pruning(self, model, pruning_ratio=0.5):\n",
        "        \"\"\"Apply unstructured magnitude-based pruning\"\"\"\n",
        "        model_pruned = deepcopy(model).to(self.device)\n",
        "\n",
        "        # Collect all linear layers\n",
        "        modules_to_prune = []\n",
        "        for name, module in model_pruned.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                modules_to_prune.append((module, 'weight'))\n",
        "\n",
        "        if not modules_to_prune:\n",
        "            print(\"Warning: No Linear layers found for pruning\")\n",
        "            return model_pruned\n",
        "\n",
        "        # Apply global unstructured pruning\n",
        "        prune.global_unstructured(\n",
        "            modules_to_prune,\n",
        "            pruning_method=prune.L1Unstructured,\n",
        "            amount=pruning_ratio,\n",
        "        )\n",
        "\n",
        "        # Make pruning permanent\n",
        "        for module, param_name in modules_to_prune:\n",
        "            prune.remove(module, param_name)\n",
        "\n",
        "        return model_pruned\n",
        "\n",
        "    def structured_pruning(self, model, pruning_ratio=0.5):\n",
        "        \"\"\"Apply structured pruning (remove entire neurons/channels)\"\"\"\n",
        "        model_pruned = deepcopy(model).to(self.device)\n",
        "\n",
        "        # Apply structured pruning to linear layers\n",
        "        for name, module in model_pruned.named_modules():\n",
        "            if isinstance(module, nn.Linear) and hasattr(module, 'weight'):\n",
        "                # Calculate L1 norm for each output neuron\n",
        "                weight = module.weight.data\n",
        "                l1_norm = torch.norm(weight, p=1, dim=1)\n",
        "\n",
        "                # Determine number of neurons to prune\n",
        "                num_neurons = weight.size(0)\n",
        "                num_to_prune = int(num_neurons * pruning_ratio)\n",
        "\n",
        "                if num_to_prune > 0 and num_to_prune < num_neurons:\n",
        "                    try:\n",
        "                        # Apply structured pruning\n",
        "                        prune.ln_structured(module, name='weight', amount=num_to_prune, n=1, dim=0)\n",
        "                        prune.remove(module, 'weight')\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Could not apply structured pruning to {name}: {e}\")\n",
        "\n",
        "        return model_pruned\n",
        "\n",
        "    def magnitude_based_pruning(self, model, pruning_ratio=0.5):\n",
        "        \"\"\"Apply magnitude-based pruning with different thresholds\"\"\"\n",
        "        model_pruned = deepcopy(model).to(self.device)\n",
        "\n",
        "        # Collect all weights\n",
        "        all_weights = []\n",
        "        for param in model_pruned.parameters():\n",
        "            if param.requires_grad:\n",
        "                all_weights.extend(param.data.abs().flatten().tolist())\n",
        "\n",
        "        if not all_weights:\n",
        "            print(\"Warning: No trainable parameters found\")\n",
        "            return model_pruned\n",
        "\n",
        "        # Calculate threshold based on magnitude\n",
        "        all_weights = torch.tensor(all_weights, device=self.device)\n",
        "        threshold = torch.quantile(all_weights, pruning_ratio)\n",
        "\n",
        "        # Apply pruning based on magnitude threshold\n",
        "        for param in model_pruned.parameters():\n",
        "            if param.requires_grad:\n",
        "                mask = param.data.abs() > threshold\n",
        "                param.data *= mask.float()\n",
        "\n",
        "        return model_pruned\n",
        "\n",
        "    def random_pruning(self, model, pruning_ratio=0.5):\n",
        "        \"\"\"Apply random pruning for comparison\"\"\"\n",
        "        model_pruned = deepcopy(model).to(self.device)\n",
        "\n",
        "        modules_to_prune = []\n",
        "        for name, module in model_pruned.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                modules_to_prune.append((module, 'weight'))\n",
        "\n",
        "        if not modules_to_prune:\n",
        "            print(\"Warning: No Linear layers found for random pruning\")\n",
        "            return model_pruned\n",
        "\n",
        "        # Apply random pruning\n",
        "        prune.global_unstructured(\n",
        "            modules_to_prune,\n",
        "            pruning_method=prune.RandomUnstructured,\n",
        "            amount=pruning_ratio,\n",
        "        )\n",
        "\n",
        "        # Make pruning permanent\n",
        "        for module, param_name in modules_to_prune:\n",
        "            prune.remove(module, param_name)\n",
        "\n",
        "        return model_pruned\n",
        "\n",
        "    def evaluate_model(self, model, test_loader, criterion):\n",
        "        \"\"\"Evaluate model performance with device consistency\"\"\"\n",
        "        model, _ = self.ensure_device_consistency(model, None)\n",
        "        model.eval()\n",
        "\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in test_loader:\n",
        "                # Move data to device - handle custom data objects\n",
        "                if hasattr(data, 'to') and callable(getattr(data, 'to')):\n",
        "                    data = data.to(self.device)\n",
        "                elif hasattr(data, '__dict__'):\n",
        "                    # For custom objects, move tensor attributes to device\n",
        "                    for attr_name in ['x', 'edge_index', 'batch', 'y']:\n",
        "                        if hasattr(data, attr_name):\n",
        "                            attr_value = getattr(data, attr_name)\n",
        "                            if torch.is_tensor(attr_value):\n",
        "                                setattr(data, attr_name, attr_value.to(self.device))\n",
        "\n",
        "                x, edge_index, batch, y = data.x, data.edge_index, data.batch, data.y\n",
        "\n",
        "                try:\n",
        "                    outputs = model(x, edge_index, batch)\n",
        "                    loss = criterion(outputs, y)\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += y.size(0)\n",
        "                    correct += (predicted == y).sum().item()\n",
        "\n",
        "                    all_preds.extend(predicted.cpu().numpy())\n",
        "                    all_labels.extend(y.cpu().numpy())\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during evaluation: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if total == 0:\n",
        "            return {\n",
        "                'accuracy': 0.0,\n",
        "                'loss': float('inf'),\n",
        "                'f1_score': 0.0,\n",
        "                'precision': 0.0,\n",
        "                'recall': 0.0\n",
        "            }\n",
        "\n",
        "        accuracy = correct / total\n",
        "        avg_loss = total_loss / len(test_loader) if len(test_loader) > 0 else float('inf')\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        try:\n",
        "            f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "            precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "            recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not calculate metrics: {e}\")\n",
        "            f1 = precision = recall = 0.0\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'loss': avg_loss,\n",
        "            'f1_score': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        }\n",
        "\n",
        "    def compare_pruning_methods(self, original_model, test_loader, sample_data,\n",
        "                              pruning_ratios=[0.1, 0.3, 0.5, 0.7, 0.9]):\n",
        "        \"\"\"Compare different pruning methods with device consistency\"\"\"\n",
        "        criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "\n",
        "        # Ensure original model is on correct device\n",
        "        original_model = original_model.to(self.device)\n",
        "\n",
        "        pruning_methods = {\n",
        "            'Original': lambda m, r: m,\n",
        "            'Unstructured': self.unstructured_pruning,\n",
        "            'Structured': self.structured_pruning,\n",
        "            'Magnitude-based': self.magnitude_based_pruning,\n",
        "            'Random': self.random_pruning\n",
        "        }\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for method_name, pruning_func in pruning_methods.items():\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Testing {method_name} Pruning\")\n",
        "            print(f\"{'='*50}\")\n",
        "\n",
        "            if method_name == 'Original':\n",
        "                ratios_to_test = [0.0]\n",
        "            else:\n",
        "                ratios_to_test = pruning_ratios\n",
        "\n",
        "            for ratio in ratios_to_test:\n",
        "                print(f\"\\nPruning ratio: {ratio}\")\n",
        "\n",
        "                try:\n",
        "                    # Apply pruning\n",
        "                    if method_name == 'Original':\n",
        "                        pruned_model = deepcopy(original_model).to(self.device)\n",
        "                    else:\n",
        "                        pruned_model = pruning_func(original_model, ratio)\n",
        "\n",
        "                    # Measure metrics\n",
        "                    start_time = time.time()\n",
        "\n",
        "                    # Model size and parameters\n",
        "                    model_size = self.get_model_size(pruned_model)\n",
        "                    total_params, non_zero_params = self.count_parameters(pruned_model)\n",
        "                    sparsity = 1 - (non_zero_params / total_params) if total_params > 0 else 0\n",
        "\n",
        "                    # Memory usage\n",
        "                    cpu_mem, gpu_mem = self.get_memory_usage()\n",
        "\n",
        "                    # Inference time\n",
        "                    inference_time = self.measure_inference_time(pruned_model, sample_data)\n",
        "\n",
        "                    # Model performance\n",
        "                    performance = self.evaluate_model(pruned_model, test_loader, criterion)\n",
        "\n",
        "                    # Compilation time\n",
        "                    compile_time = time.time() - start_time\n",
        "\n",
        "                    result = {\n",
        "                        'Method': method_name,\n",
        "                        'Pruning_Ratio': ratio,\n",
        "                        'Model_Size_MB': model_size,\n",
        "                        'Total_Parameters': total_params,\n",
        "                        'Non_Zero_Parameters': non_zero_params,\n",
        "                        'Sparsity': sparsity,\n",
        "                        'CPU_Memory_MB': cpu_mem,\n",
        "                        'GPU_Memory_MB': gpu_mem,\n",
        "                        'Inference_Time_ms': inference_time,\n",
        "                        'Compile_Time_s': compile_time,\n",
        "                        'Accuracy': performance['accuracy'],\n",
        "                        'F1_Score': performance['f1_score'],\n",
        "                        'Precision': performance['precision'],\n",
        "                        'Recall': performance['recall'],\n",
        "                        'Loss': performance['loss']\n",
        "                    }\n",
        "\n",
        "                    results.append(result)\n",
        "\n",
        "                    print(f\"  Model Size: {model_size:.2f} MB\")\n",
        "                    print(f\"  Sparsity: {sparsity:.2%}\")\n",
        "                    print(f\"  Accuracy: {performance['accuracy']:.4f}\")\n",
        "                    print(f\"  F1-Score: {performance['f1_score']:.4f}\")\n",
        "                    print(f\"  Inference Time: {inference_time:.2f} ms\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {method_name} with ratio {ratio}: {e}\")\n",
        "                    continue\n",
        "                finally:\n",
        "                    # Clean up\n",
        "                    if 'pruned_model' in locals():\n",
        "                        del pruned_model\n",
        "                    gc.collect()\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def create_visualizations(self, results_df):\n",
        "        \"\"\"Create comprehensive visualizations\"\"\"\n",
        "        if results_df.empty:\n",
        "            print(\"No results to visualize\")\n",
        "            return None\n",
        "\n",
        "        plt.style.use('default')\n",
        "        fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Accuracy vs Pruning Ratio\n",
        "        ax1 = plt.subplot(3, 3, 1)\n",
        "        for method in results_df['Method'].unique():\n",
        "            method_data = results_df[results_df['Method'] == method]\n",
        "            plt.plot(method_data['Pruning_Ratio'], method_data['Accuracy'],\n",
        "                    marker='o', linewidth=2, label=method)\n",
        "        plt.xlabel('Pruning Ratio')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Accuracy vs Pruning Ratio')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. Model Size vs Pruning Ratio\n",
        "        ax2 = plt.subplot(3, 3, 2)\n",
        "        for method in results_df['Method'].unique():\n",
        "            method_data = results_df[results_df['Method'] == method]\n",
        "            plt.plot(method_data['Pruning_Ratio'], method_data['Model_Size_MB'],\n",
        "                    marker='s', linewidth=2, label=method)\n",
        "        plt.xlabel('Pruning Ratio')\n",
        "        plt.ylabel('Model Size (MB)')\n",
        "        plt.title('Model Size vs Pruning Ratio')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. Inference Time vs Pruning Ratio\n",
        "        ax3 = plt.subplot(3, 3, 3)\n",
        "        for method in results_df['Method'].unique():\n",
        "            method_data = results_df[results_df['Method'] == method]\n",
        "            plt.plot(method_data['Pruning_Ratio'], method_data['Inference_Time_ms'],\n",
        "                    marker='^', linewidth=2, label=method)\n",
        "        plt.xlabel('Pruning Ratio')\n",
        "        plt.ylabel('Inference Time (ms)')\n",
        "        plt.title('Inference Time vs Pruning Ratio')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. Sparsity vs Accuracy\n",
        "        ax4 = plt.subplot(3, 3, 4)\n",
        "        for method in results_df['Method'].unique():\n",
        "            if method != 'Original':\n",
        "                method_data = results_df[results_df['Method'] == method]\n",
        "                plt.scatter(method_data['Sparsity'], method_data['Accuracy'],\n",
        "                           s=100, alpha=0.7, label=method)\n",
        "        plt.xlabel('Sparsity')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Sparsity vs Accuracy Trade-off')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 5. F1-Score Comparison\n",
        "        ax5 = plt.subplot(3, 3, 5)\n",
        "        pruning_50 = results_df[results_df['Pruning_Ratio'] == 0.5]\n",
        "        if not pruning_50.empty:\n",
        "            methods = pruning_50['Method'].tolist()\n",
        "            f1_scores = pruning_50['F1_Score'].tolist()\n",
        "            bars = plt.bar(methods, f1_scores, alpha=0.7, color=plt.cm.Set3(range(len(methods))))\n",
        "            plt.ylabel('F1-Score')\n",
        "            plt.title('F1-Score Comparison (50% Pruning)')\n",
        "            plt.xticks(rotation=45)\n",
        "            for bar, score in zip(bars, f1_scores):\n",
        "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                        f'{score:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # 6. Memory Efficiency\n",
        "        ax6 = plt.subplot(3, 3, 6)\n",
        "        for method in results_df['Method'].unique():\n",
        "            method_data = results_df[results_df['Method'] == method]\n",
        "            plt.plot(method_data['Pruning_Ratio'], method_data['CPU_Memory_MB'],\n",
        "                    marker='d', linewidth=2, label=method)\n",
        "        plt.xlabel('Pruning Ratio')\n",
        "        plt.ylabel('CPU Memory (MB)')\n",
        "        plt.title('Memory Usage vs Pruning Ratio')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 7. Accuracy vs Model Size (Efficiency Plot)\n",
        "        ax7 = plt.subplot(3, 3, 7)\n",
        "        for method in results_df['Method'].unique():\n",
        "            method_data = results_df[results_df['Method'] == method]\n",
        "            plt.scatter(method_data['Model_Size_MB'], method_data['Accuracy'],\n",
        "                       s=100, alpha=0.7, label=method)\n",
        "        plt.xlabel('Model Size (MB)')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Accuracy vs Model Size (Efficiency)')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # 8. Comprehensive Performance Heatmap\n",
        "        ax8 = plt.subplot(3, 3, 8)\n",
        "        pivot_data = results_df.pivot_table(\n",
        "            values='Accuracy',\n",
        "            index='Method',\n",
        "            columns='Pruning_Ratio',\n",
        "            fill_value=np.nan\n",
        "        )\n",
        "        if not pivot_data.empty:\n",
        "            sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd',\n",
        "                       cbar_kws={'label': 'Accuracy'})\n",
        "        plt.title('Accuracy Heatmap by Method and Pruning Ratio')\n",
        "\n",
        "        # 9. Pareto Frontier (Accuracy vs Compression)\n",
        "        ax9 = plt.subplot(3, 3, 9)\n",
        "        original_data = results_df[results_df['Method'] == 'Original']\n",
        "        if not original_data.empty:\n",
        "            original_size = original_data['Model_Size_MB'].iloc[0]\n",
        "\n",
        "            for method in results_df['Method'].unique():\n",
        "                if method != 'Original':\n",
        "                    method_data = results_df[results_df['Method'] == method]\n",
        "                    compression_ratio = original_size / method_data['Model_Size_MB']\n",
        "                    plt.scatter(compression_ratio, method_data['Accuracy'],\n",
        "                               s=100, alpha=0.7, label=method)\n",
        "\n",
        "            plt.xlabel('Compression Ratio')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.title('Pareto Frontier: Accuracy vs Compression')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def generate_report(self, results_df):\n",
        "        \"\"\"Generate comprehensive comparison report\"\"\"\n",
        "        if results_df.empty:\n",
        "            print(\"No results to report\")\n",
        "            return results_df\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"COMPREHENSIVE PRUNING COMPARISON REPORT\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Overall statistics\n",
        "        print(f\"\\nTested {len(results_df['Method'].unique())} pruning methods\")\n",
        "        print(f\"Pruning ratios: {sorted(results_df['Pruning_Ratio'].unique())}\")\n",
        "\n",
        "        # Best performers at different pruning ratios\n",
        "        for ratio in [0.3, 0.5, 0.7]:\n",
        "            ratio_data = results_df[results_df['Pruning_Ratio'] == ratio]\n",
        "            if not ratio_data.empty:\n",
        "                best_accuracy = ratio_data.loc[ratio_data['Accuracy'].idxmax()]\n",
        "                efficiency_metric = ratio_data['Accuracy'] / ratio_data['Model_Size_MB']\n",
        "                best_efficiency = ratio_data.loc[efficiency_metric.idxmax()]\n",
        "\n",
        "                print(f\"\\n--- At {ratio*100}% Pruning ---\")\n",
        "                print(f\"Best Accuracy: {best_accuracy['Method']} \"\n",
        "                      f\"({best_accuracy['Accuracy']:.4f})\")\n",
        "                print(f\"Best Efficiency: {best_efficiency['Method']} \"\n",
        "                      f\"(Acc: {best_efficiency['Accuracy']:.4f}, \"\n",
        "                      f\"Size: {best_efficiency['Model_Size_MB']:.2f} MB)\")\n",
        "\n",
        "        # Method comparison summary\n",
        "        print(f\"\\n--- Method Summary ---\")\n",
        "        for method in results_df['Method'].unique():\n",
        "            method_data = results_df[results_df['Method'] == method]\n",
        "            avg_accuracy = method_data['Accuracy'].mean()\n",
        "            avg_size = method_data['Model_Size_MB'].mean()\n",
        "            avg_inference = method_data['Inference_Time_ms'].mean()\n",
        "\n",
        "            print(f\"{method}:\")\n",
        "            print(f\"  Avg Accuracy: {avg_accuracy:.4f}\")\n",
        "            print(f\"  Avg Model Size: {avg_size:.2f} MB\")\n",
        "            print(f\"  Avg Inference Time: {avg_inference:.2f} ms\")\n",
        "\n",
        "        # Detailed comparison table\n",
        "        print(f\"\\n--- Detailed Results ---\")\n",
        "        print(results_df.round(4).to_string(index=False))\n",
        "\n",
        "        return results_df\n",
        "\n",
        "\n",
        "# Fixed usage example with proper batch handling\n",
        "def create_fixed_dummy_data(model_config, batch_size=32):\n",
        "    \"\"\"Create properly structured dummy data\"\"\"\n",
        "\n",
        "    # Create graph data that will result in the correct batch size\n",
        "    num_graphs = batch_size\n",
        "    nodes_per_graph = 10\n",
        "    total_nodes = num_graphs * nodes_per_graph\n",
        "\n",
        "    # Node features\n",
        "    x = torch.randn(total_nodes, model_config['num_features'])\n",
        "\n",
        "    # Create edges within each graph\n",
        "    edge_list = []\n",
        "    for i in range(num_graphs):\n",
        "        start_node = i * nodes_per_graph\n",
        "        end_node = start_node + nodes_per_graph\n",
        "        # Create a simple connected graph for each batch element\n",
        "        for j in range(start_node, end_node - 1):\n",
        "            edge_list.append([j, j + 1])\n",
        "            edge_list.append([j + 1, j])  # bidirectional\n",
        "\n",
        "    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    # Create batch indices\n",
        "    batch = torch.repeat_interleave(torch.arange(num_graphs), nodes_per_graph)\n",
        "\n",
        "    # Create labels for each graph\n",
        "    y = torch.randint(0, model_config['num_classes'], (num_graphs,))\n",
        "\n",
        "    return x, edge_index, batch, y\n",
        "\n",
        "\n",
        "# Example usage with the fix\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming you have a GINModel class defined\n",
        "    # You'll need to replace this with your actual model class\n",
        "\n",
        "    model_config = {\n",
        "        'num_features': 128,\n",
        "        'hidden_dim': 128,\n",
        "        'num_classes': 2,\n",
        "        'num_layers': 4,\n",
        "        'dropout': 0.5,\n",
        "        'global_pool': 'max',\n",
        "        'eps': 0.0,\n",
        "        'train_eps': True,\n",
        "        'batch_norm': False,\n",
        "        'layer_norm': True\n",
        "    }\n",
        "\n",
        "    # Create device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Create original model (you need to implement this)\n",
        "    # original_model = GINModel(**model_config).to(device)\n",
        "\n",
        "    # Create properly structured dummy data\n",
        "    batch_size = 32\n",
        "    x, edge_index, batch, y = create_fixed_dummy_data(model_config, batch_size)\n",
        "    sample_data = (x, edge_index, batch)\n",
        "\n",
        "    # Create dummy test loader with proper batch handling\n",
        "    class FixedDummyData:\n",
        "        def __init__(self, x, edge_index, batch, y):\n",
        "            self.x = x\n",
        "            self.edge_index = edge_index\n",
        "            self.batch = batch\n",
        "            self.y = y\n",
        "\n",
        "    # Create multiple batches for testing\n",
        "    dummy_test_loader = []\n",
        "    for _ in range(10):\n",
        "        x_batch, edge_index_batch, batch_batch, y_batch = create_fixed_dummy_data(model_config, batch_size)\n",
        "        dummy_test_loader.append(FixedDummyData(x_batch, edge_index_batch, batch_batch, y_batch))\n",
        "\n",
        "    # Initialize pruning comparator\n",
        "    comparator = PruningComparator(model_config, device)\n",
        "\n",
        "    print(\"Fixed dummy data created successfully!\")\n",
        "    print(f\"Sample data shapes:\")\n",
        "    print(f\"  x: {x.shape}\")\n",
        "    print(f\"  edge_index: {edge_index.shape}\")\n",
        "    print(f\"  batch: {batch.shape}\")\n",
        "    print(f\"  y: {y.shape}\")\n",
        "    print(f\"  Unique batch values: {torch.unique(batch)}\")\n",
        "\n",
        "# Now you can run the comparison with your actual model:\n",
        "results_df = comparator.compare_pruning_methods(\n",
        "    original_model,\n",
        "    dummy_test_loader,\n",
        "    sample_data,\n",
        "    pruning_ratios=[0.1, 0.3, 0.5, 0.7, 0.9]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvDFIx0P614Z",
        "outputId": "53d23251-a8fc-447a-c1bd-cdf1751a98d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PruningComparator initialized with device: cuda\n",
            "Fixed dummy data created successfully!\n",
            "Sample data shapes:\n",
            "  x: torch.Size([320, 128])\n",
            "  edge_index: torch.Size([2, 576])\n",
            "  batch: torch.Size([320])\n",
            "  y: torch.Size([32])\n",
            "  Unique batch values: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n",
            "\n",
            "==================================================\n",
            "Testing Original Pruning\n",
            "==================================================\n",
            "\n",
            "Pruning ratio: 0.0\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 0.39%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3127\n",
            "  Inference Time: 2.42 ms\n",
            "\n",
            "==================================================\n",
            "Testing Unstructured Pruning\n",
            "==================================================\n",
            "\n",
            "Pruning ratio: 0.1\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 10.23%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3127\n",
            "  Inference Time: 4.74 ms\n",
            "\n",
            "Pruning ratio: 0.3\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 29.92%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3127\n",
            "  Inference Time: 2.63 ms\n",
            "\n",
            "Pruning ratio: 0.5\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 49.62%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3235\n",
            "  Inference Time: 2.41 ms\n",
            "\n",
            "Pruning ratio: 0.7\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 69.31%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3127\n",
            "  Inference Time: 3.08 ms\n",
            "\n",
            "Pruning ratio: 0.9\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 89.00%\n",
            "  Accuracy: 0.4781\n",
            "  F1-Score: 0.3370\n",
            "  Inference Time: 3.03 ms\n",
            "\n",
            "==================================================\n",
            "Testing Structured Pruning\n",
            "==================================================\n",
            "\n",
            "Pruning ratio: 0.1\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 9.60%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3127\n",
            "  Inference Time: 3.36 ms\n",
            "\n",
            "Pruning ratio: 0.3\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 29.56%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3127\n",
            "  Inference Time: 3.77 ms\n",
            "\n",
            "Pruning ratio: 0.5\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 49.62%\n",
            "  Accuracy: 0.4875\n",
            "  F1-Score: 0.3367\n",
            "  Inference Time: 2.29 ms\n",
            "\n",
            "Pruning ratio: 0.7\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 68.81%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3127\n",
            "  Inference Time: 2.31 ms\n",
            "\n",
            "Pruning ratio: 0.9\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 88.77%\n",
            "  Accuracy: 0.5188\n",
            "  F1-Score: 0.3544\n",
            "  Inference Time: 2.37 ms\n",
            "\n",
            "==================================================\n",
            "Testing Magnitude-based Pruning\n",
            "==================================================\n",
            "\n",
            "Pruning ratio: 0.1\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 10.00%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3127\n",
            "  Inference Time: 2.28 ms\n",
            "\n",
            "Pruning ratio: 0.3\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 30.00%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3127\n",
            "  Inference Time: 2.40 ms\n",
            "\n",
            "Pruning ratio: 0.5\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 50.00%\n",
            "  Accuracy: 0.4875\n",
            "  F1-Score: 0.3465\n",
            "  Inference Time: 2.42 ms\n",
            "\n",
            "Pruning ratio: 0.7\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 70.00%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3127\n",
            "  Inference Time: 2.48 ms\n",
            "\n",
            "Pruning ratio: 0.9\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 90.00%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3127\n",
            "  Inference Time: 2.48 ms\n",
            "\n",
            "==================================================\n",
            "Testing Random Pruning\n",
            "==================================================\n",
            "\n",
            "Pruning ratio: 0.1\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 10.23%\n",
            "  Accuracy: 0.4938\n",
            "  F1-Score: 0.3448\n",
            "  Inference Time: 2.45 ms\n",
            "\n",
            "Pruning ratio: 0.3\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 29.92%\n",
            "  Accuracy: 0.5000\n",
            "  F1-Score: 0.3957\n",
            "  Inference Time: 2.41 ms\n",
            "\n",
            "Pruning ratio: 0.5\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 49.62%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3127\n",
            "  Inference Time: 2.38 ms\n",
            "\n",
            "Pruning ratio: 0.7\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 69.31%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3182\n",
            "  Inference Time: 2.38 ms\n",
            "\n",
            "Pruning ratio: 0.9\n",
            "  Model Size: 0.51 MB\n",
            "  Sparsity: 89.00%\n",
            "  Accuracy: 0.4813\n",
            "  F1-Score: 0.3127\n",
            "  Inference Time: 2.49 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t2soOqW5b40W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}